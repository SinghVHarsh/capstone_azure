{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56c43cdf-6c33-4d57-bb2a-ca6d6b68d5e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#if we have to read the file from the dbfs uncomment this line\n",
    "#file_location='/FileStore/tables/bank_data.csv'\n",
    "#df=spark.read.format(\"csv\").option(\"header\",\"false\").load(file_location)\n",
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8454e28-4fe0-47cf-82a3-582a32b7c5ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#setting the connection from the adls using spark.conf.set here we paas the storage account name and the access\n",
    "spark.conf.set(\"fs.azure.account.key.capstoneharsh.dfs.core.windows.net\",\"YxkaFZPHRCTvkeY3N+/o5Ltm1kBn1+zl8XkacJVFm7VTJvxz0bz/FjUgCR1PrVM18OoaKmW5V11l+ASt9QClAw==\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72f0ce67-3e8f-4f2d-b164-491fd9a0e7d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-----------+-------+---------+---+----+---+---+--------+----+----+----+----+----+----+-------+----+\n| _c0|_c1|        _c2|    _c3|      _c4|_c5| _c6|_c7|_c8|     _c9|_c10|_c11|_c12|_c13|_c14|_c15|   _c16|_c17|\n+----+---+-----------+-------+---------+---+----+---+---+--------+----+----+----+----+----+----+-------+----+\n|1001| 30| unemployed|married|  primary| no|1787| no| no|cellular|  19| oct|  79|   1|  -1|   0|unknown|  no|\n|1002| 33|   services|married|secondary| no|4789|yes|yes|cellular|  11| may| 220|   1| 339|   4|failure|  no|\n|1003| 35| management| single| tertiary| no|1350|yes| no|cellular|  16| apr| 185|   1| 330|   1|failure|  no|\n|1004| 30| management|married| tertiary| no|1476|yes|yes| unknown|   3| jun| 199|   4|  -1|   0|unknown|  no|\n|1005| 59|blue-collar|married|secondary| no|   0|yes| no| unknown|   5| may| 226|   1|  -1|   0|unknown|  no|\n+----+---+-----------+-------+---------+---+----+---+---+--------+----+----+----+----+----+----+-------+----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "#reading the spark data frame from adls thrugh set configuration\n",
    "df = spark.read.csv('abfss://rawdata@capstoneharsh.dfs.core.windows.net/SinghVHarsh/capstone_azure/main/bank_data.csv'\n",
    "## If header exists uncomment line below\n",
    "##, header=True\n",
    ")\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03847262-04e2-4957-a038-0094374cfee1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#writing the table in the form of delta table to write in the bronze layer\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"bronze_csv_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91965f39-a041-46ca-915e-e134cdaa2956",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Temporary View/Table \n",
    "df.createOrReplaceTempView(\"rawview\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1696488216718024,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "execute1",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
